{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import little_mallet_wrapper as lmw\n",
    "import openpyxl\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "import xlwt\n",
    "import xlrd\n",
    "import xlutils.copy as xc\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 MALLET Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mallet = 'C:/mallet/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load dataset\n",
    "\n",
    "16张sheet，每张1大类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../huawei_appdescrip_spider/4-大类LDA-应用-华为应用市场app分类与功能描述.xls'  # CHANGE THIS TO YOUR DATASET PATH\n",
    "\n",
    "df_sheets_dict = pd.read_excel(dataset_path,sheet_name = None,encoding=\"utf-8\")\n",
    "sheets_name = list(df_sheets_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheets = [df_sheets_dict[i] for i in sheets_name] #16张表\n",
    "\n",
    "# 从表格读取数据到list，待输入LDA\n",
    "training_data_list = []\n",
    "for df_sheet in df_sheets:\n",
    "    training_data = [t for t in df_sheet['迭代去干扰词9'].tolist()] #改！！！\n",
    "    # training_data = [t for t in df_sheet['分词+去停用词+去低频词'].tolist()]\n",
    "    training_data = [str(d).strip() for d in training_data if str(d)]\n",
    "    training_data_list.append(training_data)\n",
    "\n",
    "# training_data_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sub_types = [6,6,4,4,7,4,4,5,3,6,5,3,5,4,6,4] #每大类分成几小类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 训练 LDA 主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Complete\n",
      "开始训练 1 影音娱乐 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 2 实用工具 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 3 社交通讯 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 4 教育 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 5 新闻阅读 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 6 拍摄美化 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 7 美食 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 8 出行导航 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 9 旅游住宿 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 10 购物比价 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 11 商务 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 12 儿童 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 13 金融理财 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 14 运动健康 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 15 便捷生活 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n",
      "Importing data...\n",
      "Complete\n",
      "开始训练 16 汽车 LDA模型...\n",
      "Training topic model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):    \n",
    "    num_topics = num_sub_types[i]  # CHANGE THIS TO YOUR PREFERRED NUMBER OF TOPICS\n",
    "    path_str = 'huawei_LDA_output_原分类测试/'+ sheets_name[i] + '-' + str(num_topics)\n",
    "    os.makedirs(path_str)\n",
    "\n",
    "    output_directory_path = path_str\n",
    "\n",
    "    path_to_training_data           = output_directory_path + '/training.txt'\n",
    "    path_to_formatted_training_data = output_directory_path + '/mallet.training'\n",
    "    path_to_model                   = output_directory_path + '/mallet.model.' + str(num_topics)\n",
    "    path_to_topic_keys              = output_directory_path + '/mallet.topic_keys.' + str(num_topics)\n",
    "    path_to_topic_distributions     = output_directory_path + '/mallet.topic_distributions.' + str(num_topics)\n",
    "    path_to_word_weights            = output_directory_path + '/mallet.word_weights.' + str(num_topics)\n",
    "    path_to_diagnostics             = output_directory_path + '/mallet.diagnostics.' + str(num_topics) + '.xml'\n",
    "\n",
    "    \n",
    "    lmw.import_data(path_to_mallet,\n",
    "                    path_to_training_data, #txt格式数据\n",
    "                    path_to_formatted_training_data, #MALLET 格式数据\n",
    "                    training_data_list[i]) #原始数据\n",
    "\n",
    "    print(\"开始训练\",i+1,sheets_name[i],\"LDA模型...\")\n",
    "    lmw.train_topic_model(path_to_mallet,\n",
    "                      path_to_formatted_training_data,\n",
    "                      path_to_model,\n",
    "                      path_to_topic_keys,\n",
    "                      path_to_topic_distributions,\n",
    "                      path_to_word_weights,\n",
    "                      path_to_diagnostics,\n",
    "                      num_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 doc主题分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 加载每个doc的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions_list = []\n",
    "\n",
    "for i in range(16):\n",
    "    num_topics = num_sub_types[i]  \n",
    "    output_directory_path = 'huawei_LDA_output_原分类测试/'+ sheets_name[i] + '-' + str(num_topics) \n",
    "\n",
    "    topic_distributions = lmw.load_topic_distributions(output_directory_path + '/mallet.topic_distributions.' + str(num_topics))\n",
    "    topic_distributions_list.append(topic_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)获取每个doc的的主题向量：概率从大到小排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs_max_p_all_list = []\n",
    "\n",
    "for k in range(16):\n",
    "    num_topics = num_sub_types[k]  \n",
    "    output_directory_path = 'huawei_LDA_output_原分类测试/'+ sheets_name[k] + '-' + str(num_topics) \n",
    "    # print(sheets_name[k])\n",
    "\n",
    "    num_max_p_topics =  num_sub_types[k]#取每个doc最可能的topic个数\n",
    "\n",
    "    indexs_max_p_all = [] #存所有doc的主题向量，主题向量=[(主题,概率),()]\n",
    "\n",
    "    for i in range(len(training_data_list[k])): #遍历所有doc\n",
    "        p_topics = copy.deepcopy(topic_distributions_list[k][i]) #每个doc的概率list\n",
    "        indexs_max_p = [] #存1个doc最可能的topic下标\n",
    "\n",
    "        for j in range(num_max_p_topics): #找最大值\n",
    "            max_p = max(p_topics)\n",
    "            index_max_p = p_topics.index(max_p)\n",
    "\n",
    "            # indexs_max_p.append((index_max_p,max_p)) #(主题下标，概率) \n",
    "            indexs_max_p.append((index_max_p,round(max_p,4))) #(主题下标，概率)\n",
    "            # indexs_max_p.append((index_max_p,round(max_p,4)*100)) #(主题下标，概率) 概率单位%\n",
    "            p_topics[index_max_p] = -1 #最大值改0\n",
    "\n",
    "        indexs_max_p_all.append(indexs_max_p)\n",
    "    \n",
    "    indexs_max_p_all_list.append(indexs_max_p_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)追加写入主题向量到excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_excel_xls_append(path, id_sheet,value):\n",
    "    index = len(value)  # 获取需要写入数据的行数\n",
    "\n",
    "    workbook = xlrd.open_workbook(path)  # 打开工作簿\n",
    "    sheets = workbook.sheet_names()  # 获取工作簿中的所有表格\n",
    "    worksheet = workbook.sheet_by_name(sheets[id_sheet])  # 获取工作簿中所有表格中的的第一个表格\n",
    "\n",
    "    rows_old = worksheet.nrows  # 获取表格中已存在的数据的行数\n",
    "    cols_old = worksheet.ncols  # 获取表格中已存在的数据的行数\n",
    "\n",
    "    new_workbook = xc.copy(workbook)  # 将xlrd对象拷贝转化为xlwt对象\n",
    "    new_worksheet = new_workbook.get_sheet(id_sheet)  # 获取转化后工作簿中的第一个表格\n",
    "\n",
    "    for i in range(0, index):\n",
    "        for j in range(0, len(value[i])):\n",
    "            new_worksheet.write(i, j+cols_old, str(value[i][j]))  # 追加写入数据，注意是从i+rows_old行开始写入\n",
    "    new_workbook.save(path)  # 保存工作簿\n",
    "    print(\"写入完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入主题向量: 1 影音娱乐 ...\n",
      "写入完成！\n",
      "写入主题向量: 2 实用工具 ...\n",
      "写入完成！\n",
      "写入主题向量: 3 社交通讯 ...\n",
      "写入完成！\n",
      "写入主题向量: 4 教育 ...\n",
      "写入完成！\n",
      "写入主题向量: 5 新闻阅读 ...\n",
      "写入完成！\n",
      "写入主题向量: 6 拍摄美化 ...\n",
      "写入完成！\n",
      "写入主题向量: 7 美食 ...\n",
      "写入完成！\n",
      "写入主题向量: 8 出行导航 ...\n",
      "写入完成！\n",
      "写入主题向量: 9 旅游住宿 ...\n",
      "写入完成！\n",
      "写入主题向量: 10 购物比价 ...\n",
      "写入完成！\n",
      "写入主题向量: 11 商务 ...\n",
      "写入完成！\n",
      "写入主题向量: 12 儿童 ...\n",
      "写入完成！\n",
      "写入主题向量: 13 金融理财 ...\n",
      "写入完成！\n",
      "写入主题向量: 14 运动健康 ...\n",
      "写入完成！\n",
      "写入主题向量: 15 便捷生活 ...\n",
      "写入完成！\n",
      "写入主题向量: 16 汽车 ...\n",
      "写入完成！\n"
     ]
    }
   ],
   "source": [
    "for k in range(16):\n",
    "    num_topics = num_sub_types[k]\n",
    "    print(\"写入主题向量:\",k+1,sheets_name[k],\"...\")\n",
    "\n",
    "    data = []\n",
    "    data.append([\"主题\",\"概率\"])\n",
    "\n",
    "    temp_indexs_max_p_all = indexs_max_p_all_list[k]\n",
    "    maxnum = len(temp_indexs_max_p_all)\n",
    "\n",
    "    for i in range(maxnum): \n",
    "        topic_vector = copy.deepcopy(temp_indexs_max_p_all[i]) #遍历每个app的主题向量=[(主题,概率),()]\n",
    "        temp = []\n",
    "\n",
    "        for j in range(num_topics): #遍历主题向量内每个元组，共num_topics个\n",
    "            if topic_vector[j][1] <= 0.05: \n",
    "                topic_vector = topic_vector[:j]\n",
    "                break\n",
    "            else:\n",
    "                temp.append(topic_vector[j][0])\n",
    "                temp.append(topic_vector[j][1])\n",
    "\n",
    "        data.append(temp)\n",
    "        \n",
    "    write_excel_xls_append(\"LDA-应用-主题向量-原分类测试-789对比.xls\",k,data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)属于指定topic的概率最高的doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989 传承 非凡 德系 技术 品质 顺应 中国 消费者 个性 自我 美好 方式 需求 捷达 致力 品质 汽车 品牌 典范 汽车 品牌 全新子 品牌 姿态 消费者 开启 汽车 境界 german quality demands modern consumers a lifestyle jetta quality brand opening door a people places 捷达 品牌 车型 延续 可靠性 品质 珍贵 特质 全新 设计 领先 核心 科技 车型 选择 中国 消费者 个性 表达 彰显 年轻 方式 捷达 品牌 年轻 用户 购 需求 纯正 德系 品牌 love jetta brand german reliability safety quality but a fresh design solid core technologies a variety model options jetta meet demands consumers lifestyle jetta a brand german customers 你好 捷达 携 全新 车型 家族 hello i jetta coming a family car models 捷达 版块 线 联网 投屏 键 智联 精彩 出行 你好 捷达 语音 控 快速 响应 聪明 操控 体验 智慧 加油 提供 省心 场景化 钱 电台 账号 同步 旅途 愉快 版块 提供\n",
      "\n",
      "0.9983 您好 智驾蜂 款 爱车 贴近 时代 主旨 倡导 环保 节能 车载 设备 创新 技术 环境 开放 包容 态度 建立 共享 生态 环境 汽车物 联网   致力 解决 搭 汽车物 联网 智驾蜂 汽车物 联网 智驾蜂 2.0 3.0 硬件 设备 连接 路 环境 建立 共享 生态 物联 产品 围绕 环保 节能 创新 开放 共享 生态 物联 设备 颠覆 接入点 创新 模式 秉承 开放 包容 态度 构建 全民 共享 立体 生态 物联 智驾蜂 汽车物 联网 智驾蜂 车载 终端 读取 汽车 行车 电脑 相关 传感 设备 通信 云端 存储 分析 用户 端 后台 后台 四 层 物联网 基础   收集 层 车辆 运行 车辆 车况 车辆 定位 驾驶员 习惯 车主 习惯 车主 消费 习惯 车辆 生命 周期   层 驾驶 习惯 车辆 运行 车辆 尾气 排放 指数 建立 中国 驾驶 减 低 尾气 排放 标准 环保 节能 减排 车辆 运行 车辆 定位 完善 智慧 城市 交通 分析 建立 道路 交通 潮汐 系统 引流 导向 车流 交通 堵塞 分流 建立 配合 城市 公共 设施 配套 车辆 运行 车辆 定位 驾驶员 习惯 建立 中国式 ubi 互助 体系 车主 习惯 车主 消费 习惯 建立 双 omo 商城 车辆 生命 周期 车辆 年审 运营 规范 生命 周期 健康 档案 建立 中国式 电子 公路 体系 公共 停车 配套 体系 车辆 体系 驾驶员 体系   市场 层 深化 4S 店 维修店 美容店 销售 汽车 市场 特种 私家车 企业 车辆 零 成本 少 开车 开 保养 忧 生命 周期 ubi 行驶 时间 精确 小时 智驾蜂 商城 建立 双 omo 生态 租车 汽车 社区 超市 微贷 社区化 保 公共 停车 驾校 培训 体系 驾驶员 生命 周期 监控 体系 公交车 驾驶员 系统 校园 周边 车辆 系统 驾驶 驾驶 汽车物 联网 硬件 开发 后台 建设 汽车 市场 生态 搭建 内容 主界面 页面 显示 增加 智驾蜂 福利 抽奖 增加 每日 签到 增加 优惠券\n",
      "\n",
      "0.998 车速表 款 简单 易行 汽车 测速 致力 车主 软件 行车 记录仪 里程表 车速表 车主 仪表盘 车主 测量 精确 速度 跟踪 距离 提供 保障 担心 事故 行车 拍摄 路上 发生 事情 车主 速度 测量 速度 距离 行车 记录仪 发生 交通 事故 时 提供 证据 速度 信息 平视 显示器 模式 测速 测 距离 速度 指示 车辆 行驶 速度 手机 观看 平均 速度 车速 用于 查看 行驶 速度 软件 支持 两 单位 测量 单位 选择 mph km 选择 合适 车型 里程表 里程 跟踪器 测 距离 行车 记录仪 袖珍 行车 记录仪 仪表盘 行车 记录仪 拍摄 事故 平视 显示器 模式 测速 行车 记录仪 手机 放在 仪表盘 测量 速度 距离 行车 信息 跟踪 驾驶 过程 获取 速度 距离 开车 骑 自行车 慢 跑 时 控制 行车 记录仪 测速 神器 事故 期间 提供 信息 里程表 跟踪 里程 车速表 测速 行车 记录仪 拍摄 道路 视屏 定期 内容 仅 订阅 时 可用 订阅 前 阅读 订阅 订阅 续订 帐户 退订> 订阅期 结束 前 24 小时 扣除 订阅费> 第一 次 提供 免费 试用期 试用期 结束 前 未 退订 订阅 续订 付费 订阅 帐户 扣除 订阅费 购买 前 阅读\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_top_docs返回元组(概率，训练的doc)\n",
    "for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=0, n=3):\n",
    "    print(round(p, 4), d) #概率取小数点后4位\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 主题词\n",
    "\n",
    "- 为每个主题加载概率最大单词集：每个主题最多20个词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys_list = []\n",
    "for i in range(16):\n",
    "    num_topics = num_sub_types[i]  \n",
    "    output_directory_path = 'huawei_LDA_output_原分类测试/'+ sheets_name[i] + '-' + str(num_topics) \n",
    "    topic_keys_path = output_directory_path + '/mallet.topic_keys.' + str(num_topics)\n",
    "    topic_keys = [line.split('\\t')[2].split() for line in open(topic_keys_path, 'r',encoding=\"utf-8\")] \n",
    "\n",
    "    topic_keys_list.append(topic_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载每个topic的最高概率主题词分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以概率排序的主题词写入完成！\n"
     ]
    }
   ],
   "source": [
    "topic_keys_list = []\n",
    "topic_word_probability_dict_16 = []\n",
    "\n",
    "newfile = open('huawei_LDA_output_原分类测试/keywords_and_p_all_topics.txt', 'w',encoding=\"UTF-8\")\n",
    "\n",
    "for i in range(16):\n",
    "    num_topics = num_sub_types[i]  \n",
    "    output_directory_path = 'huawei_LDA_output_原分类测试/'+ sheets_name[i] + '-' + str(num_topics) \n",
    "    word_weight_path = output_directory_path + '/mallet.word_weights.' + str(num_topics)\n",
    "\n",
    "    topic_word_weight_dict = defaultdict(lambda: defaultdict(float))\n",
    "    topic_sum_dict = defaultdict(float)\n",
    "    with open(word_weight_path,'r',encoding=\"utf-8\") as f:       \n",
    "        for _line in f:        \n",
    "            _topic, _word, _weight = _line.split('\\t')\n",
    "            topic_word_weight_dict[_topic][_word] = float(_weight)\n",
    "            topic_sum_dict[_topic] += float(_weight)\n",
    "\n",
    "    topic_word_probability_dict = defaultdict(lambda: defaultdict(float))\n",
    "    for _topic, _word_weight_dict in topic_word_weight_dict.items():\n",
    "        for _word, _weight in _word_weight_dict.items():\n",
    "            topic_word_probability_dict[int(_topic)][_word] = _weight / topic_sum_dict[_topic]\n",
    "\n",
    "    topic_word_probability_dict_16.append(topic_word_probability_dict)\n",
    "\n",
    "    # print(\"=====================\",sheets_name[i],\"共\",len(topic_word_probability_dict),\"个topic==================\")\n",
    "    newfile.writelines('========')\n",
    "    newfile.writelines(sheets_name[i])\n",
    "    newfile.writelines(':')\n",
    "    newfile.writelines(str(len(topic_word_probability_dict)))\n",
    "    newfile.writelines('个topic')\n",
    "    newfile.writelines('========')\n",
    "    newfile.writelines('\\n')\n",
    "\n",
    "    for _topic, _word_probability_dict in topic_word_probability_dict.items():\n",
    "        # print('Topic', _topic)\n",
    "        newfile.writelines('Topic')\n",
    "        newfile.writelines('\\t')\n",
    "        newfile.writelines(str(_topic))\n",
    "        newfile.writelines('\\n')\n",
    "        # 要显示的个数 改！！\n",
    "        for _word, _probability in sorted(_word_probability_dict.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "            p = round(_probability, 4)\n",
    "            # print(p,'\\t', _word)\n",
    "            newfile.writelines(str(p))\n",
    "            newfile.writelines('\\t')\n",
    "            newfile.writelines(str(_word))\n",
    "            newfile.writelines('\\n')\n",
    "            \n",
    "            \n",
    "        # print()\n",
    "        newfile.writelines('-------------------')\n",
    "        newfile.writelines('\\n')\n",
    "\n",
    "newfile.close()\n",
    "print(\"以概率排序的主题词写入完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 主题分布散度\n",
    "\n",
    "- 计算两个topic分布之间的 Jensen-Shannon 散度：相似度越高，散度越小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题散度写入完成！\n"
     ]
    }
   ],
   "source": [
    "divergence_file = open('huawei_LDA_output_原分类测试/divergence_topic_words.txt', 'w',encoding=\"UTF-8\")\n",
    "for i in range(16):\n",
    "    num_topics = num_sub_types[i]\n",
    "\n",
    "    divergence_file.writelines('========')\n",
    "    divergence_file.writelines(sheets_name[i])\n",
    "    divergence_file.writelines(':')\n",
    "    divergence_file.writelines(str(len(topic_word_probability_dict_16[i])))\n",
    "    divergence_file.writelines('个topic')\n",
    "    divergence_file.writelines('========')\n",
    "    divergence_file.writelines('\\n')\n",
    "            \n",
    "    for j in range(num_topics):\n",
    "        k = j+1\n",
    "        while(k<=num_topics-1):\n",
    "            divergence = lmw.get_js_divergence_topics(j,k, topic_word_probability_dict_16[i])\n",
    "            divergence_file.writelines(str(j)) \n",
    "            divergence_file.writelines('\\t')\n",
    "            divergence_file.writelines(str(k))\n",
    "            divergence_file.writelines('\\t')\n",
    "            divergence_file.writelines('d = ')\n",
    "            divergence_file.writelines(str(round(divergence,4)))\n",
    "\n",
    "            divergence_file.writelines('\\n')\n",
    "            k += 1\n",
    "        \n",
    "divergence_file.close()\n",
    "print(\"主题散度写入完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot topics by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据集包含一段时间的分类标签，则创建标签 x 主题的热图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = ['John Keats', 'Emily Dickinson', 'William Butler Yeats', 'Christina Rossetti']\n",
    "\n",
    "lmw.plot_categories_by_topics_heatmap(authors,\n",
    "                                      topic_distributions,\n",
    "                                      topic_keys, \n",
    "                                      output_directory_path + '/categories_by_topics.pdf',\n",
    "                                      target_labels=target_labels,\n",
    "                                      dim=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据集包含一些时间的分类标签，则创建一组箱线图，每个主题一个图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_labels = ['John Keats', 'Emily Dickinson', 'William Butler Yeats', 'Christina Rossetti']\n",
    "\n",
    "for _topic_index in range(0, len(topic_keys)):\n",
    "    \n",
    "    lmw.plot_categories_by_topic_boxplots(authors,\n",
    "                                          topic_distributions,\n",
    "                                          topic_keys, \n",
    "                                          _topic_index,\n",
    "                                          output_path=output_directory_path + '/boxplot.' + str(_topic_index) + '.pdf',\n",
    "                                          target_labels=target_labels,\n",
    "                                          dim=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot topics over document time\n",
    "\n",
    "Splits each training document into N chunks, infers the topics for those chunks (using the pre-trained topic model), and plots mean topic probabilities over document time.\n",
    "\n",
    "In the example shown, it looks like poems often start with physical descriptions and end with abstract and romantic language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个数据集，将每个文档分成num_chunks个doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divided_documents, document_ids, times = lmw.divide_training_data(training_data,\n",
    "                                                                  num_chunks=10)\n",
    "\n",
    "len(divided_documents), len(document_ids), len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_divided_training_data           = output_directory_path + '/training.split.txt'\n",
    "path_to_divided_formatted_training_data = output_directory_path + '/mallet.split.training'\n",
    "path_to_divided_topic_distributions     = output_directory_path + '/mallet.split.topic_distributions.' + str(num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmw.import_data(path_to_mallet,\n",
    "                path_to_divided_training_data,\n",
    "                path_to_divided_formatted_training_data,\n",
    "                divided_documents,\n",
    "                use_pipe_from=path_to_formatted_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **模型使用**\n",
    "使用已在另一组文档上训练的模型获取一组新文档的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmw.infer_topics(path_to_mallet,\n",
    "                 path_to_model,\n",
    "                 path_to_divided_formatted_training_data,\n",
    "                 path_to_divided_topic_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练主题模型后加载每个文档的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lmw.load_topic_distributions(path_to_divided_topic_distributions)\n",
    "\n",
    "len(topic_distributions), len(topic_distributions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建线图，每个主题一个，显示文档段的平均主题概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _topic_index in range(0, len(topic_keys)):\n",
    "    lmw.plot_topics_over_time(topic_distributions, topic_keys, times, _topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "575720ce556736a07aaf3f83c0863d0caa8a0eed3061f43770eda7f92a76a775"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('python3.7env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
