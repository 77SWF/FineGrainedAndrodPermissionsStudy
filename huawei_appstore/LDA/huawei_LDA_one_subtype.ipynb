{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import little_mallet_wrapper as lmw\n",
    "import openpyxl\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "import xlwt\n",
    "import xlrd\n",
    "import xlutils.copy as xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 MALLET Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mallet = 'C:/mallet/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load dataset\n",
    "\n",
    "16张sheet，每张1大类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../huawei_appdescrip_spider/4-大类LDA-应用-华为应用市场app分类与功能描述.xls'  # CHANGE THIS TO YOUR DATASET PATH\n",
    "\n",
    "df_sheets_dict = pd.read_excel(dataset_path,sheet_name = None,encoding=\"utf-8\")\n",
    "sheets_name = list(df_sheets_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>分类</th>\n",
       "      <th>子分类</th>\n",
       "      <th>应用名</th>\n",
       "      <th>分词+去停用词+去低频词</th>\n",
       "      <th>迭代去干扰词</th>\n",
       "      <th>迭代去干扰词2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>超级粘土史莱姆制作</td>\n",
       "      <td>提供 各式各样 软泥 做 想不到 发挥 创造性 捏 造型 经典 史莱姆 粘液 模拟 制作 打...</td>\n",
       "      <td>提供 各式各样 软泥 做 发挥 创造性 捏 造型 经典 史莱姆 粘液 模拟 制作 打造 喜欢...</td>\n",
       "      <td>提供 各式各样 软泥 做 发挥 创造性 捏 造型 经典 史莱姆 粘液 模拟 制作 打造 喜欢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>托卡迷你城市城堡</td>\n",
       "      <td>来到 托卡 小镇 迷你 城市 城堡 玩耍 朋友 邀请 家里 参观 游玩 朋友 托卡 小镇 四...</td>\n",
       "      <td>来到 托卡 小镇 迷你 城市 城堡 玩耍 朋友 邀请 家里 参观 游玩 朋友 托卡 小镇 四...</td>\n",
       "      <td>来到 托卡 小镇 迷你 城市 城堡 玩耍 朋友 邀请 家里 参观 游玩 朋友 托卡 小镇 四...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>填色小公主</td>\n",
       "      <td>填色 公主 款 简单 益智 软件 锻炼 孩子 思维 能力 家长 解压 神器 随意 选择 喜欢...</td>\n",
       "      <td>填色 公主 款 简单 益智 软件 锻炼 孩子 思维 能力 家长 解压 神器 随意 选择 喜欢...</td>\n",
       "      <td>填色 公主 款 简单 益智 软件 锻炼 孩子 思维 能力 家长 解压 神器 随意 选择 喜欢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>讯飞儿童手表</td>\n",
       "      <td>款 讯飞 儿童 手表 硬件 设备 配套  讯飞 儿童 手表 绑定 手表 便捷 设置 手表 状...</td>\n",
       "      <td>款 讯飞 儿童 手表 硬件 设备 配套  讯飞 儿童 手表 绑定 手表 便捷 设置 手表 状...</td>\n",
       "      <td>款 讯飞 儿童 手表 硬件 设备 配套  讯飞 儿童 手表 绑定 手表 便捷 设置 手表 状...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>美食烹饪厨房</td>\n",
       "      <td>美食 烹饪 厨房 款 任由 烹饪 天下 美食 餐厅 经营 类 带领 走 进 场 天下 美食 ...</td>\n",
       "      <td>美食 烹饪 厨房 款 任由 烹饪 天下 美食 餐厅 经营 类 带领 走 进 场 天下 美食 ...</td>\n",
       "      <td>美食 烹饪 厨房 款 任由 烹饪 天下 美食 餐厅 经营 类 带领 走 进 场 天下 美食 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>女生美甲涂色化妆大全</td>\n",
       "      <td>时尚 女孩 有趣 美甲 沙龙 提升 美甲 艺术 设计 水平 享受 舒服 手 部 SPA 洗 ...</td>\n",
       "      <td>时尚 女孩 有趣 美甲 沙龙 提升 美甲 艺术 设计 水平 享受 舒服 手 部 spa 洗 ...</td>\n",
       "      <td>时尚 女孩 有趣 美甲 沙龙 提升 美甲 艺术 设计 水平 享受 舒服 手 部 洗 手上 灰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>换装少女</td>\n",
       "      <td>装 公主 款 公主 形象 装扮 打扮 益智 衣服 配饰 发型 流行 风格 公主 装扮 发型 ...</td>\n",
       "      <td>装 公主 款 公主 形象 装扮 打扮 益智 衣服 配饰 发型 流行 风格 公主 装扮 发型 ...</td>\n",
       "      <td>装 公主 款 公主 形象 装扮 打扮 益智 衣服 配饰 发型 流行 风格 公主 装扮 发型 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>中英文绘本故事</td>\n",
       "      <td>快来 中英文 绘本 故事 每晚 宝宝 讲 两 睡 前 故事 中英文 绘本 故事 款 美式 分...</td>\n",
       "      <td>快来 中英文 绘本 故事 每晚 宝宝 讲 两 睡 前 故事 中英文 绘本 故事 款 美式 分...</td>\n",
       "      <td>快来 中英文 绘本 故事 每晚 宝宝 讲 两 睡 前 故事 中英文 绘本 故事 款 美式 分...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>组装超级机器人</td>\n",
       "      <td>电流 抓住 机会 屏幕 机器人 组装 新颖 组装 玩法 简单 眼疾手快 变形 金刚 形象 探...</td>\n",
       "      <td>电流 抓住 机会 屏幕 机器人 组装 新颖 组装 玩法 简单 眼疾手快 变形 金刚 形象 探...</td>\n",
       "      <td>电流 抓住 机会 屏幕 机器人 组装 新颖 组装 玩法 简单 眼疾手快 变形 金刚 形象 探...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>儿童欢乐小火车</td>\n",
       "      <td>迷宫 行走 专家 逻辑 思维 观察力 闯 下班 时间 适合 休息 放松 勇 闯 迷宫 简单 ...</td>\n",
       "      <td>迷宫 行走 专家 逻辑 思维 观察力 闯 下班 时间 适合 休息 放松 勇 闯 迷宫 简单 ...</td>\n",
       "      <td>迷宫 行走 专家 逻辑 思维 观察力 闯 下班 时间 适合 休息 放松 勇 闯 迷宫 简单 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>儿童</td>\n",
       "      <td>儿童教育</td>\n",
       "      <td>我的减肥日记</td>\n",
       "      <td>越来越 选择 健康 方式 健身房 里 锻炼 减肥 体形 拥有 间 健身房 每位 顾客 塑身 ...</td>\n",
       "      <td>选择 健康 方式 健身房 里 锻炼 减肥 体形 拥有 间 健身房 每位 顾客 塑身 减肥 间...</td>\n",
       "      <td>选择 健康 方式 健身房 里 锻炼 减肥 体形 拥有 间 健身房 每位 顾客 塑身 减肥 间...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      分类   子分类         应用名                                       分词+去停用词+去低频词  \\\n",
       "92    儿童  儿童教育   超级粘土史莱姆制作  提供 各式各样 软泥 做 想不到 发挥 创造性 捏 造型 经典 史莱姆 粘液 模拟 制作 打...   \n",
       "190   儿童  儿童教育    托卡迷你城市城堡  来到 托卡 小镇 迷你 城市 城堡 玩耍 朋友 邀请 家里 参观 游玩 朋友 托卡 小镇 四...   \n",
       "1716  儿童  儿童教育       填色小公主  填色 公主 款 简单 益智 软件 锻炼 孩子 思维 能力 家长 解压 神器 随意 选择 喜欢...   \n",
       "1637  儿童  儿童教育      讯飞儿童手表  款 讯飞 儿童 手表 硬件 设备 配套  讯飞 儿童 手表 绑定 手表 便捷 设置 手表 状...   \n",
       "461   儿童  儿童教育      美食烹饪厨房  美食 烹饪 厨房 款 任由 烹饪 天下 美食 餐厅 经营 类 带领 走 进 场 天下 美食 ...   \n",
       "241   儿童  儿童教育  女生美甲涂色化妆大全  时尚 女孩 有趣 美甲 沙龙 提升 美甲 艺术 设计 水平 享受 舒服 手 部 SPA 洗 ...   \n",
       "1350  儿童  儿童教育        换装少女  装 公主 款 公主 形象 装扮 打扮 益智 衣服 配饰 发型 流行 风格 公主 装扮 发型 ...   \n",
       "1694  儿童  儿童教育     中英文绘本故事  快来 中英文 绘本 故事 每晚 宝宝 讲 两 睡 前 故事 中英文 绘本 故事 款 美式 分...   \n",
       "848   儿童  儿童教育     组装超级机器人  电流 抓住 机会 屏幕 机器人 组装 新颖 组装 玩法 简单 眼疾手快 变形 金刚 形象 探...   \n",
       "1409  儿童  儿童教育     儿童欢乐小火车  迷宫 行走 专家 逻辑 思维 观察力 闯 下班 时间 适合 休息 放松 勇 闯 迷宫 简单 ...   \n",
       "769   儿童  儿童教育      我的减肥日记  越来越 选择 健康 方式 健身房 里 锻炼 减肥 体形 拥有 间 健身房 每位 顾客 塑身 ...   \n",
       "\n",
       "                                                 迭代去干扰词  \\\n",
       "92    提供 各式各样 软泥 做 发挥 创造性 捏 造型 经典 史莱姆 粘液 模拟 制作 打造 喜欢...   \n",
       "190   来到 托卡 小镇 迷你 城市 城堡 玩耍 朋友 邀请 家里 参观 游玩 朋友 托卡 小镇 四...   \n",
       "1716  填色 公主 款 简单 益智 软件 锻炼 孩子 思维 能力 家长 解压 神器 随意 选择 喜欢...   \n",
       "1637  款 讯飞 儿童 手表 硬件 设备 配套  讯飞 儿童 手表 绑定 手表 便捷 设置 手表 状...   \n",
       "461   美食 烹饪 厨房 款 任由 烹饪 天下 美食 餐厅 经营 类 带领 走 进 场 天下 美食 ...   \n",
       "241   时尚 女孩 有趣 美甲 沙龙 提升 美甲 艺术 设计 水平 享受 舒服 手 部 spa 洗 ...   \n",
       "1350  装 公主 款 公主 形象 装扮 打扮 益智 衣服 配饰 发型 流行 风格 公主 装扮 发型 ...   \n",
       "1694  快来 中英文 绘本 故事 每晚 宝宝 讲 两 睡 前 故事 中英文 绘本 故事 款 美式 分...   \n",
       "848   电流 抓住 机会 屏幕 机器人 组装 新颖 组装 玩法 简单 眼疾手快 变形 金刚 形象 探...   \n",
       "1409  迷宫 行走 专家 逻辑 思维 观察力 闯 下班 时间 适合 休息 放松 勇 闯 迷宫 简单 ...   \n",
       "769   选择 健康 方式 健身房 里 锻炼 减肥 体形 拥有 间 健身房 每位 顾客 塑身 减肥 间...   \n",
       "\n",
       "                                                迭代去干扰词2  \n",
       "92    提供 各式各样 软泥 做 发挥 创造性 捏 造型 经典 史莱姆 粘液 模拟 制作 打造 喜欢...  \n",
       "190   来到 托卡 小镇 迷你 城市 城堡 玩耍 朋友 邀请 家里 参观 游玩 朋友 托卡 小镇 四...  \n",
       "1716  填色 公主 款 简单 益智 软件 锻炼 孩子 思维 能力 家长 解压 神器 随意 选择 喜欢...  \n",
       "1637  款 讯飞 儿童 手表 硬件 设备 配套  讯飞 儿童 手表 绑定 手表 便捷 设置 手表 状...  \n",
       "461   美食 烹饪 厨房 款 任由 烹饪 天下 美食 餐厅 经营 类 带领 走 进 场 天下 美食 ...  \n",
       "241   时尚 女孩 有趣 美甲 沙龙 提升 美甲 艺术 设计 水平 享受 舒服 手 部 洗 手上 灰...  \n",
       "1350  装 公主 款 公主 形象 装扮 打扮 益智 衣服 配饰 发型 流行 风格 公主 装扮 发型 ...  \n",
       "1694  快来 中英文 绘本 故事 每晚 宝宝 讲 两 睡 前 故事 中英文 绘本 故事 款 美式 分...  \n",
       "848   电流 抓住 机会 屏幕 机器人 组装 新颖 组装 玩法 简单 眼疾手快 变形 金刚 形象 探...  \n",
       "1409  迷宫 行走 专家 逻辑 思维 观察力 闯 下班 时间 适合 休息 放松 勇 闯 迷宫 简单 ...  \n",
       "769   选择 健康 方式 健身房 里 锻炼 减肥 体形 拥有 间 健身房 每位 顾客 塑身 减肥 间...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试读某一sheet\n",
    "df_one_sheet = df_sheets_dict[sheets_name[11]]\n",
    "df_one_sheet.sample(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2139"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从表格读取数据到list，待输入LDA\n",
    "training_data = [t for t in df_one_sheet['迭代去干扰词2'].tolist()]\n",
    "training_data = [str(d).strip() for d in training_data if str(d)]\n",
    "\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1052\n",
      "Mean Number of Words per Document: 101.5\n",
      "Vocabulary Size: 12092\n"
     ]
    }
   ],
   "source": [
    "#显示有关训练数据集的基本统计信息\n",
    "lmw.print_dataset_stats(training_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 训练 LDA 主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3  # CHANGE THIS TO YOUR PREFERRED NUMBER OF TOPICS\n",
    "\n",
    "output_directory_path = 'huawei_LDA_output_原分类测试/儿童-3'  # CHANGE THIS TO YOUR OUTPUT DIRECTORY\n",
    "# output_directory_path = 'huawei_LDA_output_原分类测试/儿童-3' + str(num_topics) # CHANGE THIS TO YOUR OUTPUT DIRECTORY\n",
    "\n",
    "path_to_training_data           = output_directory_path + '/training.txt'\n",
    "path_to_formatted_training_data = output_directory_path + '/mallet.training'\n",
    "path_to_model                   = output_directory_path + '/mallet.model.' + str(num_topics)\n",
    "path_to_topic_keys              = output_directory_path + '/mallet.topic_keys.' + str(num_topics)\n",
    "path_to_topic_distributions     = output_directory_path + '/mallet.topic_distributions.' + str(num_topics)\n",
    "path_to_word_weights            = output_directory_path + '/mallet.word_weights.' + str(num_topics)\n",
    "path_to_diagnostics             = output_directory_path + '/mallet.diagnostics.' + str(num_topics) + '.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lmw.import_data(path_to_mallet,\n",
    "                path_to_training_data, #txt格式数据\n",
    "                path_to_formatted_training_data, #MALLET 格式数据\n",
    "                training_data) #原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training topic model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "lmw.train_topic_model(path_to_mallet,\n",
    "                      path_to_formatted_training_data,\n",
    "                      path_to_model,\n",
    "                      path_to_topic_keys,\n",
    "                      path_to_topic_distributions,\n",
    "                      path_to_word_weights,\n",
    "                      path_to_diagnostics,\n",
    "                      num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 主题词\n",
    "\n",
    "- 为每个主题加载概率最大单词集：每个主题最多20个词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 客户端 随时随地 个性化 互联网 音乐人 应有尽有 爱好者 电视台 有限公司 沉浸式\n",
      "1 \t 节拍器 调音器 尤克里里 爱好者 吉他谱 架子鼓 演奏家 五线谱 MIDI 随时随地\n",
      "2 \t 电视剧 爱奇艺 优酷全网 NBA 应有尽有 TVB 真人秀 客户端 长津湖 机器人\n",
      "3 \t 随时随地 高颜值 面对面 KTV 应有尽有 陌生人 排位赛 零距离 演唱会 脱口秀\n",
      "4 \t 收音机 Bose 广场舞 脱口秀 CNC 爱好者 黄梅戏 随时随地 鬼吹灯 广播剧\n",
      "5 \t 播放器 娃娃机 均衡器 音频格式 马戏团 推币机 提取器 DSD AAC 随时随地\n"
     ]
    }
   ],
   "source": [
    "topic_keys_path = output_directory_path + '/mallet.topic_keys.' + str(num_topics)\n",
    "topic_keys = [line.split('\\t')[2].split() for line in open(topic_keys_path, 'r',encoding=\"utf-8\")] \n",
    "\n",
    "for i, t in enumerate(topic_keys):\n",
    "    print(i, '\\t', ' '.join(t[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载每个topic的最高概率主题词分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================共 6 个topic==================\n",
      "Topic 0\n",
      "0.0333 \t 客户端\n",
      "0.0247 \t 随时随地\n",
      "0.0229 \t 个性化\n",
      "0.0201 \t 互联网\n",
      "0.0178 \t 音乐人\n",
      "\n",
      "Topic 1\n",
      "0.0781 \t 节拍器\n",
      "0.0552 \t 调音器\n",
      "0.0414 \t 尤克里里\n",
      "0.0263 \t 爱好者\n",
      "0.0152 \t 吉他谱\n",
      "\n",
      "Topic 2\n",
      "0.0321 \t 电视剧\n",
      "0.022 \t 爱奇艺\n",
      "0.0173 \t 优酷全网\n",
      "0.0155 \t NBA\n",
      "0.0119 \t 应有尽有\n",
      "\n",
      "Topic 3\n",
      "0.0772 \t 随时随地\n",
      "0.0762 \t 高颜值\n",
      "0.0142 \t 面对面\n",
      "0.0132 \t 应有尽有\n",
      "0.0132 \t KTV\n",
      "\n",
      "Topic 4\n",
      "0.0457 \t 收音机\n",
      "0.0204 \t Bose\n",
      "0.0189 \t 广场舞\n",
      "0.0146 \t 脱口秀\n",
      "0.0117 \t CNC\n",
      "\n",
      "Topic 5\n",
      "0.0926 \t 播放器\n",
      "0.0274 \t 娃娃机\n",
      "0.0237 \t 均衡器\n",
      "0.0179 \t 音频格式\n",
      "0.0122 \t 马戏团\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topic_word_probability_dict = lmw.load_topic_word_distributions(output_directory_path + '/mallet.word_weights.' + str(num_topics))\n",
    "\n",
    "word_weight_path = output_directory_path + '/mallet.word_weights.' + str(num_topics)\n",
    "\n",
    "topic_word_weight_dict = defaultdict(lambda: defaultdict(float))\n",
    "topic_sum_dict = defaultdict(float)\n",
    "with open(word_weight_path,'r',encoding=\"utf-8\") as f:       \n",
    "    for _line in f:        \n",
    "        _topic, _word, _weight = _line.split('\\t')\n",
    "        topic_word_weight_dict[_topic][_word] = float(_weight)\n",
    "        topic_sum_dict[_topic] += float(_weight)\n",
    "\n",
    "topic_word_probability_dict = defaultdict(lambda: defaultdict(float))\n",
    "for _topic, _word_weight_dict in topic_word_weight_dict.items():\n",
    "    for _word, _weight in _word_weight_dict.items():\n",
    "        topic_word_probability_dict[int(_topic)][_word] = _weight / topic_sum_dict[_topic]\n",
    "\n",
    "print(\"=====================共\",len(topic_word_probability_dict),\"个topic==================\")\n",
    "\n",
    "for _topic, _word_probability_dict in topic_word_probability_dict.items():\n",
    "    print('Topic', _topic)\n",
    "    for _word, _probability in sorted(_word_probability_dict.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(round(_probability, 4), '\\t', _word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 doc主题分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 加载每个doc的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lmw.load_topic_distributions(output_directory_path + '/mallet.topic_distributions.' + str(num_topics))\n",
    "\n",
    "len(topic_distributions), len(topic_distributions[0])\n",
    "\n",
    "assert(len(topic_distributions) == len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)获取每个doc的的主题向量：概率从大到小排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_max_p_topics = 2 #取每个doc最可能的topic个数\n",
    "num_max_p_topics = num_topics #取每个doc最可能的topic个数\n",
    "\n",
    "indexs_max_p_all = [] #存所有doc的主题向量，主题向量=[(主题,概率),()]\n",
    "\n",
    "for i in range(len(training_data)): #遍历所有doc\n",
    "    p_topics = copy.deepcopy(topic_distributions[i]) #每个doc的概率list\n",
    "    indexs_max_p = [] #存1个doc最可能的topic下标\n",
    "\n",
    "    for j in range(num_max_p_topics): #找最大值\n",
    "        max_p = max(p_topics)\n",
    "        index_max_p = p_topics.index(max_p)\n",
    "        # indexs_max_p.append((index_max_p,max_p)) #(主题下标，概率) \n",
    "        indexs_max_p.append((index_max_p,round(max_p,4))) #(主题下标，概率)\n",
    "        # indexs_max_p.append((index_max_p,round(max_p,4)*100)) #(主题下标，概率) 概率单位%\n",
    "        p_topics[index_max_p] = -1 #最大值改0\n",
    "\n",
    "    indexs_max_p_all.append(indexs_max_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)写入主题向量到csv，去掉概率小于0.05的topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============写入各app的主题向量==============\n",
      "==============写入完成==============\n"
     ]
    }
   ],
   "source": [
    "print(\"==============写入各app的主题向量==============\")\n",
    "\n",
    "f = open(\"主题向量.csv\",'w',encoding='utf-8',newline='')\n",
    "save_csv = csv.writer(f)\n",
    "\n",
    "maxnum = len(indexs_max_p_all)\n",
    "for i in range(maxnum): \n",
    "    topic_vector = copy.deepcopy(indexs_max_p_all[i]) #遍历每个app的主题向量=[(主题,概率),()]\n",
    "\n",
    "    for j in range(num_topics): #遍历主题向量内每个元组，共num_topics个\n",
    "        if topic_vector[j][1] <= 0.05: \n",
    "            topic_vector = topic_vector[:j]\n",
    "            # print(topic_vector)\n",
    "            break\n",
    "        \n",
    "\n",
    "    save_csv.writerow([v for v in topic_vector]) #避免csv分隔\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(\"==============写入完成==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)追加写入主题向量到excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_excel_xls_append(path, id_sheet,value):\n",
    "    index = len(value)  # 获取需要写入数据的行数\n",
    "\n",
    "    workbook = xlrd.open_workbook(path)  # 打开工作簿\n",
    "    sheets = workbook.sheet_names()  # 获取工作簿中的所有表格\n",
    "    worksheet = workbook.sheet_by_name(sheets[id_sheet])  # 获取工作簿中所有表格中的的第一个表格\n",
    "\n",
    "    rows_old = worksheet.nrows  # 获取表格中已存在的数据的行数\n",
    "    cols_old = worksheet.ncols  # 获取表格中已存在的数据的行数\n",
    "\n",
    "    new_workbook = xc.copy(workbook)  # 将xlrd对象拷贝转化为xlwt对象\n",
    "    new_worksheet = new_workbook.get_sheet(id_sheet)  # 获取转化后工作簿中的第一个表格\n",
    "\n",
    "    for i in range(0, index):\n",
    "        for j in range(0, len(value[i])):\n",
    "            new_worksheet.write(i, j+cols_old, str(value[i][j]))  # 追加写入数据，注意是从i+rows_old行开始写入\n",
    "    new_workbook.save(path)  # 保存工作簿\n",
    "    print(\"xls格式表格【追加】写入数据成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============写入各app的主题向量==============\n",
      "xls格式表格【追加】写入数据成功！\n",
      "==============写入完成==============\n"
     ]
    }
   ],
   "source": [
    "print(\"==============写入各app的主题向量==============\")\n",
    "\n",
    "data = []\n",
    "data.append([\"主题\",\"概率\"])\n",
    "\n",
    "maxnum = len(indexs_max_p_all)\n",
    "for i in range(maxnum): \n",
    "    topic_vector = copy.deepcopy(indexs_max_p_all[i]) #遍历每个app的主题向量=[(主题,概率),()]\n",
    "    temp = []\n",
    "\n",
    "    for j in range(num_topics): #遍历主题向量内每个元组，共num_topics个\n",
    "        if topic_vector[j][1] <= 0.05: \n",
    "            topic_vector = topic_vector[:j]\n",
    "            break\n",
    "        else:\n",
    "            temp.append(topic_vector[j][0])\n",
    "            temp.append(topic_vector[j][1])\n",
    "\n",
    "    data.append(temp)\n",
    "    \n",
    "write_excel_xls_append(\"大类LDA-应用-主题向量.xls\",0,data)\n",
    "\n",
    "print(\"==============写入完成==============\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)属于指定topic的概率最高的doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_docs返回元组(概率，训练的doc)\n",
    "for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=0, n=3):\n",
    "    print(round(p, 4), d) #概率取小数点后4位\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 主题分布散度\n",
    "\n",
    "- 计算两个topic分布之间的 Jensen-Shannon 散度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmw.get_js_divergence_topics(0, 7, topic_word_probability_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot topics by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据集包含一段时间的分类标签，则创建标签 x 主题的热图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = ['John Keats', 'Emily Dickinson', 'William Butler Yeats', 'Christina Rossetti']\n",
    "\n",
    "lmw.plot_categories_by_topics_heatmap(authors,\n",
    "                                      topic_distributions,\n",
    "                                      topic_keys, \n",
    "                                      output_directory_path + '/categories_by_topics.pdf',\n",
    "                                      target_labels=target_labels,\n",
    "                                      dim=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据集包含一些时间的分类标签，则创建一组箱线图，每个主题一个图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_labels = ['John Keats', 'Emily Dickinson', 'William Butler Yeats', 'Christina Rossetti']\n",
    "\n",
    "for _topic_index in range(0, len(topic_keys)):\n",
    "    \n",
    "    lmw.plot_categories_by_topic_boxplots(authors,\n",
    "                                          topic_distributions,\n",
    "                                          topic_keys, \n",
    "                                          _topic_index,\n",
    "                                          output_path=output_directory_path + '/boxplot.' + str(_topic_index) + '.pdf',\n",
    "                                          target_labels=target_labels,\n",
    "                                          dim=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot topics over document time\n",
    "\n",
    "Splits each training document into N chunks, infers the topics for those chunks (using the pre-trained topic model), and plots mean topic probabilities over document time.\n",
    "\n",
    "In the example shown, it looks like poems often start with physical descriptions and end with abstract and romantic language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个数据集，将每个文档分成num_chunks个doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divided_documents, document_ids, times = lmw.divide_training_data(training_data,\n",
    "                                                                  num_chunks=10)\n",
    "\n",
    "len(divided_documents), len(document_ids), len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_divided_training_data           = output_directory_path + '/training.split.txt'\n",
    "path_to_divided_formatted_training_data = output_directory_path + '/mallet.split.training'\n",
    "path_to_divided_topic_distributions     = output_directory_path + '/mallet.split.topic_distributions.' + str(num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmw.import_data(path_to_mallet,\n",
    "                path_to_divided_training_data,\n",
    "                path_to_divided_formatted_training_data,\n",
    "                divided_documents,\n",
    "                use_pipe_from=path_to_formatted_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **模型使用**\n",
    "使用已在另一组文档上训练的模型获取一组新文档的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmw.infer_topics(path_to_mallet,\n",
    "                 path_to_model,\n",
    "                 path_to_divided_formatted_training_data,\n",
    "                 path_to_divided_topic_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练主题模型后加载每个文档的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lmw.load_topic_distributions(path_to_divided_topic_distributions)\n",
    "\n",
    "len(topic_distributions), len(topic_distributions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建线图，每个主题一个，显示文档段的平均主题概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _topic_index in range(0, len(topic_keys)):\n",
    "    lmw.plot_topics_over_time(topic_distributions, topic_keys, times, _topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "575720ce556736a07aaf3f83c0863d0caa8a0eed3061f43770eda7f92a76a775"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('python3.7env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
